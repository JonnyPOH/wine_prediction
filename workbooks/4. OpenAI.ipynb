{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variables\n",
    "OPENAI_API_KEY = \"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\"\n",
    "client = OpenAI(api_key = OPENAI_API_KEY)\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt\n",
    "\n",
    "\n",
    "Prepare and upload training data \\\n",
    "Train a new fine-tuned model \\\n",
    "Evaluate results and go back to step 1 if needed \\\n",
    "Use your fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and upload training data ( create a diverse set of demonstration conversations that are similar to the conversations you will ask the model to respond to at inference time in production )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>variety</th>\n",
       "      <th>description_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>good dry creek zin robust dry spicy really get...</td>\n",
       "      <td>89</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>France</td>\n",
       "      <td>herbaceous character make wine seem rather thi...</td>\n",
       "      <td>84</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Bordeaux-style White Blend</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>little simple easy wealth raspberry strawberry...</td>\n",
       "      <td>84</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Rosé</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>dry farmed vineyard treated wild yeast minimal...</td>\n",
       "      <td>88</td>\n",
       "      <td>38.0</td>\n",
       "      <td>Petite Sirah</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>site near annapolis wine show preponderance da...</td>\n",
       "      <td>91</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Pinot Noir</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country                                        description  points  price  \\\n",
       "0      US  good dry creek zin robust dry spicy really get...      89   25.0   \n",
       "1  France  herbaceous character make wine seem rather thi...      84   20.0   \n",
       "2      US  little simple easy wealth raspberry strawberry...      84   19.0   \n",
       "3      US  dry farmed vineyard treated wild yeast minimal...      88   38.0   \n",
       "4      US  site near annapolis wine show preponderance da...      91   62.0   \n",
       "\n",
       "                      variety  description_length  \n",
       "0                   Zinfandel                 175  \n",
       "1  Bordeaux-style White Blend                 115  \n",
       "2                        Rosé                 110  \n",
       "3                Petite Sirah                 172  \n",
       "4                  Pinot Noir                 169  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "os.chdir(\"..\")\n",
    "data_path = 'datasets'\n",
    "df = pd.read_csv(os.path.join(data_path, 'wine_quality_cleaned.csv'))\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df['description_length'] = df['description'].str.len()\n",
    "# Display basic information about the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file saved: wine_classification.jsonl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define the system message\n",
    "system_message = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a wine classification assistant that predicts the country of origin based on wine features.\"\n",
    "}\n",
    "\n",
    "# Convert DataFrame to JSONL format\n",
    "jsonl_data = []\n",
    "for _, row in df.iterrows():\n",
    "    entry = {\n",
    "        \"messages\": [\n",
    "            system_message,  # The system instruction\n",
    "            {\"role\": \"user\", \"content\": f\"Price: {row['price']}, Score: {row['points']}, Review: '{row['description']}'\"},\n",
    "            {\"role\": \"assistant\", \"content\": row[\"country\"]}  # The correct country classification\n",
    "        ]\n",
    "    }\n",
    "    jsonl_data.append(entry)\n",
    "\n",
    "# Save as JSONL file\n",
    "jsonl_file_path = \"wine_classification.jsonl\"\n",
    "with open(jsonl_file_path, \"w\") as f:\n",
    "    for entry in jsonl_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"JSONL file saved: {jsonl_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test/ train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Train file saved: wine_train.jsonl, Test file saved: wine_test.jsonl'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load JSONL data from file\n",
    "jsonl_file_path = \"wine_classification.jsonl\"\n",
    "with open(jsonl_file_path, \"r\") as f:\n",
    "    jsonl_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Shuffle the data to ensure random splitting\n",
    "random.shuffle(jsonl_data)\n",
    "\n",
    "# Define split ratio (e.g., 80% train, 20% test)\n",
    "train_ratio = 0.8\n",
    "split_index = int(len(jsonl_data) * train_ratio)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_data = jsonl_data[:split_index]\n",
    "test_data = jsonl_data[split_index:]\n",
    "\n",
    "# Save training data\n",
    "train_file_path = \"wine_train.jsonl\"\n",
    "with open(train_file_path, \"w\") as f:\n",
    "    for entry in train_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Save testing data\n",
    "test_file_path = \"wine_test.jsonl\"\n",
    "with open(test_file_path, \"w\") as f:\n",
    "    for entry in test_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Provide confirmation\n",
    "{\"message\": f\"Train file saved: {train_file_path}, Test file saved: {test_file_path}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 200\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a wine classification assistant that predicts the country of origin based on wine features.'}\n",
      "{'role': 'user', 'content': \"Price: 24.0, Score: 90, Review: 'made entirely chardonnay elegant delicate winery current bubbly dry sophisticated wine subtle citrus peach apple yeast flavor never cellared sparkling wine try stashing away decade'\"}\n",
      "{'role': 'assistant', 'content': 'US'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_path = \"wine_test.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "\n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# useful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 60, 104\n",
      "mean / median: 78.795, 79.0\n",
      "p5 / p95: 68.0, 90.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 1\n",
      "mean / median: 1.0, 1.0\n",
      "p5 / p95: 1.0, 1.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~15759 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~47277 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uploading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file ID: file-UwUwrQcj1ykzv59uBUyhM8\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Upload the JSONL file\n",
    "file_upload = client.files.create(\n",
    "    file=open(\"wine_train.jsonl\", \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "file_id = file_upload.id  # Store the file ID\n",
    "print(f\"Uploaded file ID: {file_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.fine_tuning.jobs.create(\n",
    "    training_file=file_id,\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    method={\n",
    "        \"type\": \"supervised\",\n",
    "        \"supervised\": {\n",
    "            \"hyperparameters\": {\n",
    "                \"batch_size\": 25,\n",
    "                \"learning_rate_multiplier\": 1.8,\n",
    "                \"n_epochs\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Country of Origin: US\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "fine_tuned_model=\"ft:gpt-4o-mini-2024-07-18:personal::Ay5wZDtW\"\n",
    "fine_tuned_model_j=\"ft:gpt-4o-mini-2024-07-18:personal::AyGh2lZU\"\n",
    "\n",
    "wine_features = \"Price: 30, Score: 90, Review: 'Rich and full-bodied with dark fruit notes and a smooth finish.'\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=fine_tuned_model,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a wine classification assistant that predicts the country of origin based on wine features.\"},\n",
    "        {\"role\": \"user\", \"content\": wine_features}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the predicted country\n",
    "predicted_country = completion.choices[0].message.content\n",
    "\n",
    "print(f\"Predicted Country of Origin: {predicted_country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing your fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fj_job = 'ftjob-ljho4V9A6Kb8Wsmw8ukeAoZJ'\n",
    "\n",
    "fj_job_j = 'ftjob-1lAVUjpk5pirPK05680s22qA'\n",
    "\n",
    "job_details = client.fine_tuning.jobs.retrieve(fj_job)\n",
    "job_details_j = client.fine_tuning.jobs.retrieve(fj_job_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load test data\n",
    "test_file_path = \"wine_test.jsonl\"\n",
    "\n",
    "with open(test_file_path, \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Samples: 200\n",
      "Correct Predictions: 171\n",
      "Model Accuracy: 85.50%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)\n",
    "\n",
    "for entry in test_data:\n",
    "    user_input = entry[\"messages\"][1][\"content\"]  # Extract wine features\n",
    "    actual_country = entry[\"messages\"][2][\"content\"]  # Correct label\n",
    "\n",
    "    # Query fine-tuned model for prediction\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"ft:gpt-4o-mini-2024-07-18:personal::Ay5wZDtW\",  # Your fine-tuned model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a wine classification assistant that predicts the country of origin based on wine features.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract predicted country\n",
    "    predicted_country = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Compare prediction with actual label\n",
    "    if predicted_country.lower() == actual_country.lower():\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Test Samples: {total_predictions}\")\n",
    "print(f\"Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Samples: 200\n",
      "Correct Predictions: 182\n",
      "Model Accuracy: 91.00%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)\n",
    "\n",
    "for entry in test_data:\n",
    "    user_input = entry[\"messages\"][1][\"content\"]  # Extract wine features\n",
    "    actual_country = entry[\"messages\"][2][\"content\"]  # Correct label\n",
    "\n",
    "    # Query fine-tuned model for prediction\n",
    "    completion = client.chat.completions.create(\n",
    "        model=fine_tuned_model_j,  # Your fine-tuned model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a wine classification assistant that predicts the country of origin based on wine features.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract predicted country\n",
    "    predicted_country = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Compare prediction with actual label\n",
    "    if predicted_country.lower() == actual_country.lower():\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Test Samples: {total_predictions}\")\n",
    "print(f\"Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test Samples: 200\n",
      "Correct Predictions: 180\n",
      "Model Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(test_data)\n",
    "\n",
    "for entry in test_data:\n",
    "    user_input = entry[\"messages\"][1][\"content\"]  # Extract wine features\n",
    "    actual_country = entry[\"messages\"][2][\"content\"]  # Correct label\n",
    "\n",
    "    # Query fine-tuned model for prediction\n",
    "    completion = client.chat.completions.create(\n",
    "        model=fine_tuned_model_j,  # Your fine-tuned model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a wine classification assistant that predicts the country of origin based on wine features.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract predicted country\n",
    "    predicted_country = completion.choices[0].message.content.strip()\n",
    "\n",
    "    # Compare prediction with actual label\n",
    "    if predicted_country.lower() == actual_country.lower():\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Total Test Samples: {total_predictions}\")\n",
    "print(f\"Correct Predictions: {correct_predictions}\")\n",
    "print(f\"Model Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModel operation: RoleArn: Cross-account pass role is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sm \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagemaker\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mModelName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWineModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPrimaryContainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m123456789012.dkr.ecr.us-east-1.amazonaws.com/scikit-learn-inference\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModelDataUrl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://your-bucket-name/wine_model.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mExecutionRoleArn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marn:aws:iam::123456789012:role/SageMakerRole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.4/lib/python3.12/site-packages/botocore/client.py:983\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    979\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    981\u001b[0m     )\n\u001b[1;32m    982\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModel operation: RoleArn: Cross-account pass role is not allowed."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "sm = boto3.client(\"sagemaker\")\n",
    "\n",
    "response = sm.create_model(\n",
    "    ModelName=\"WineModel\",\n",
    "    PrimaryContainer={\n",
    "        \"Image\": \"123456789012.dkr.ecr.us-east-1.amazonaws.com/scikit-learn-inference\",\n",
    "        \"ModelDataUrl\": \"s3://your-bucket-name/wine_model.pkl\"\n",
    "    },\n",
    "    ExecutionRoleArn=\"arn:aws:iam::123456789012:role/SageMakerRole\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madd\u001b[49m\u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'add' is not defined"
     ]
    }
   ],
   "source": [
    "add#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
